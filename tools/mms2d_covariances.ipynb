{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly\n",
    "import plotly.figure_factory as ff\n",
    "import numpy as np\n",
    "import uuid\n",
    "import monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.datasets import *\n",
    "from utils.data_augmentation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_covariance(input_data):\n",
    "    \"\"\"\n",
    "    Compute Covariance matrix of the input data\n",
    "    \"\"\"\n",
    "    n = input_data.size(0)  # batch_size\n",
    "\n",
    "    # Check if using gpu or cpu\n",
    "    if input_data.is_cuda:\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    id_row = torch.ones(n).resize(1, n).to(device=device)\n",
    "    sum_column = torch.mm(id_row, input_data)\n",
    "    mean_column = torch.div(sum_column, n)\n",
    "    term_mul_2 = torch.mm(mean_column.t(), mean_column)\n",
    "    d_t_d = torch.mm(input_data.t(), input_data)\n",
    "    c = torch.add(d_t_d, (-1 * term_mul_2)) * 1 / (n - 1)\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vendor = \"A\"\n",
    "normalization=\"none\"\n",
    "data_mod=\"\"\n",
    "verbose=False\n",
    "\n",
    "add_depth = False\n",
    "batch_size = 1\n",
    "\n",
    "data_augmentation = \"none\"\n",
    "img_size, crop_size = 224, 224 # We will take original image not transformed one\n",
    "mask_reshape_method = \"padd\"\n",
    "\n",
    "train_aug, train_aug_img, val_aug = data_augmentation_selector(\n",
    "    data_augmentation, img_size, crop_size, mask_reshape_method, verbose=verbose\n",
    ")\n",
    "\n",
    "dataset = f\"mms_vendor{vendor}{data_mod}\"\n",
    "\n",
    "only_end = False if \"full\" in dataset else True\n",
    "unlabeled = True if \"unlabeled\" in dataset else False\n",
    "c_centre = find_values(dataset, \"centre\", int)\n",
    "c_vendor = find_values(dataset, \"vendor\", str)\n",
    "\n",
    "\n",
    "train_dataset = MMs3DDataset(\n",
    "    partition=\"Training\", transform=train_aug, img_transform=train_aug_img, \n",
    "    normalization=normalization, add_depth=add_depth, \n",
    "    is_labeled=(not unlabeled), centre=c_centre, vendor=c_vendor, \n",
    "    end_volumes=only_end, data_relative_path=\"../\"\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, pin_memory=True,\n",
    "    shuffle=False, collate_fn=train_dataset.custom_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Image] batch shape: torch.Size([1, 13, 1, 224, 224])\n",
      "[Image] first_volume: torch.Size([13, 1, 224, 224])\n",
      "[Image] first_slice: torch.Size([1, 224, 224])\n",
      "---------------------------------------------\n",
      "[Image] batch shape: torch.Size([1, 13, 1, 224, 224])\n",
      "[Image] first_volume: torch.Size([13, 1, 224, 224])\n",
      "[Image] first_slice: torch.Size([1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "slice_indx = 5\n",
    "\n",
    "batch_image = batch[\"image\"]\n",
    "print(f\"[Image] batch shape: {batch_image.shape}\")\n",
    "first_volume = batch_image[0]\n",
    "print(f\"[Image] first_volume: {first_volume.shape}\")\n",
    "first_slice = first_volume[slice_indx]\n",
    "print(f\"[Image] first_slice: {first_slice.shape}\")\n",
    "\n",
    "\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "batch_mask = batch[\"label\"]\n",
    "print(f\"[Image] batch shape: {batch_mask.shape}\")\n",
    "first_volume_mask = batch_mask[0]\n",
    "print(f\"[Image] first_volume: {first_volume_mask.shape}\")\n",
    "first_slice_mask = first_volume_mask[slice_indx]\n",
    "print(f\"[Image] first_slice: {first_slice_mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import volumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Resize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-aea62aceb104>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m Compose([\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malways_apply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m ], p=1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Resize' is not defined"
     ]
    }
   ],
   "source": [
    "Compose([\n",
    "    Resize((13,224,224), always_apply=True),\n",
    "], p=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = monai.transforms.Resize((13,1,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 224, 224])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_volume.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Input Error: Only 3D, 4D and 5D input Tensors supported (got 6D) for the modes: nearest | linear | bilinear | bicubic | trilinear (got area)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-2304dc66b02c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_volume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/monai/transforms/spatial/array.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img, mode, align_corners)\u001b[0m\n\u001b[1;32m    354\u001b[0m             )\n\u001b[1;32m    355\u001b[0m         \u001b[0mspatial_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfall_back_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         resized = _torch_interp(\n\u001b[0m\u001b[1;32m    357\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspatial_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/monai/transforms/spatial/array.py\u001b[0m in \u001b[0;36m_torch_interp\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# additional argument since torch 1.5 (to avoid warnings)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_torch_interp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecompute_scale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor)\u001b[0m\n\u001b[1;32m   3175\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample_bicubic2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msfl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msfl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3176\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3177\u001b[0;31m         raise NotImplementedError(\"Input Error: Only 3D, 4D and 5D input Tensors supported\"\n\u001b[0m\u001b[1;32m   3178\u001b[0m                                   \u001b[0;34m\" (got {}D) for the modes: nearest | linear | bilinear | bicubic | trilinear\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3179\u001b[0m                                   \" (got {})\".format(input.dim(), mode))\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Input Error: Only 3D, 4D and 5D input Tensors supported (got 6D) for the modes: nearest | linear | bilinear | bicubic | trilinear (got area)"
     ]
    }
   ],
   "source": [
    "resize(first_volume).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_2_onehot(mask, num_classes):\n",
    "    \"\"\"\n",
    "    Transform multiclass mask [1, h, w] to [num_classes, h, w]\n",
    "    \"\"\"\n",
    "    class_slice = []\n",
    "    for c_class in range(num_classes+1):\n",
    "        class_slice.append( ((mask == c_class) * 1) )\n",
    "    return torch.cat(class_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_slice_mask_onehot = cat_mask_2_onehot(first_slice_mask, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50176, 4])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_slice_mask_onehot.permute(1,2,0).view(-1, 4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9063, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0483, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0160, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0294]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_covariance( first_slice_mask_onehot.permute(1,2,0).view(-1, 4).float() )+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_list_A, img_list_A = get_data(\"A\", normalization=\"none\")\n",
    "mask_list_B, img_list_B = get_data(\"B\", normalization=\"none\")\n",
    "#img_list_C = get_data(\"C\", normalization=\"none\", data_mod=\"_unlabeled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 224, 224])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_list_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1224, -0.1344, -1.1159,  0.5518, -0.1867,  0.5311, -0.3326,  0.3444,\n",
       "         -2.7127,  0.3377]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn((2,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_covariance(torch.zeros((10,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
